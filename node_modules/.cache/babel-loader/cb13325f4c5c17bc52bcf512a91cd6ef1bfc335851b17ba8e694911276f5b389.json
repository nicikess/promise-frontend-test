{"ast":null,"code":"const participants = [\n//Participant 1\n{\n  name: \"Participant 1\",\n  userStory: \"\\\"Starts by reading artifacts silently. (00:00-06:00)\\\n    Then he goes to Celonis and looks at the activities of the most common variants in the Variant Explorer and Process Explorer to familiarize, to get an initial understanding of the sequence of the process and the most common variants. He also makes a few assumptions about what he observes, for example \\\"\\\"so from Create Fine to Payment, this means that they received the fine directly on the place, right?\\\"\\\". (06:00-09:30)\\\n    Then, he looks at the research question to see what is expected and where to focus. (09:30-10:30)\\\n    He begins the analysis focusing on the Send for Credit Collection activity in Variant Explorer, which he had identified from the activity description as one circumstance for not paying. He filters for cases containing the Send for Credit Collection activity and explores variants. He then notices that some variants still contain the Payment activity and filters for cases without Payment. (10:30-14:00)\\\n    He mentions creating an OLAP table and looks at attributes to know what to put in, but quickly abandons that goal. (14:00-15:00) \\\n    [Replanning] Then, he decides to use the conformance checker as a fast solution to find reasons.\\\n    Since he doesn\\'t have a desired model, he uses the one discovered automatically by Celonis, removes the Send for Credit Collection activity to get the desired BPMN model and uses it as input for the conformance checker. Then he scrolls through the list of violations and stops at the first one, the Send for Credit Collection activity which was the one he wanted to drill down to. He performs root cause analysis for the Send for Credit Collection activity but doesn\\'t get any specific reason. Looks also at the other violations. (15:00-21:30)\\\n    [Replanning] He remembers about temporal constraints in the artifacts.\\\n    Applies throughput time selection filters to check the 90-day temporal constraint to find the answers. Then he moves on to check the temporal constraints about the prefecture. Checks the dismissal attribute and builds an OLAP table to check for dismissed cases. He\\'s satisfied with his findings. (21:30-33:00)\\\"\",\n  pmExpertise: \"Good\",\n  analysisExpertise: \"Basic\",\n  tool: [\"Celonis\"]\n},\n//Participant 2\n{\n  name: \"Participant 2\",\n  pmExpertise: \"Average\",\n  userStory: \"\\\"Starts by reading the artifacts to get an understanding of the data (00:00-04:00) \\\n    Then he reads the guiding question (04:00-06:00). \\\n    Then he goes to Disco and looks the process map and general statistics to get a basic understanding of the process flow. (06:00 - 08:00). Then he switches between the guiding question and the artifacts attempting to find an activity that corresponds to not paying. Since he cannot find it, he looks into the attributes but then asks for help. The interviewer clarifies the question (08:00 - 13:00). \\\n    He continues searching in the attributes for something that could indicate not paying [seems not to know where to visualize attributes in Disco or raw data. He is encouraged by the interviewer]. (13:00-17:00) \\\n    Then he filters for cases that do not include the Payment activity and compares the DFG of the filtered event log in with the original one to find patterns [but he does not seem to find any] (17:00-24:30) \\\n    He asks the interviewer to clarify once more the concept of circumstance and decides that \\\"\\\"no Payment activity\\\"\\\" is then a circumstance. (24:00-27:00) \\\n    He seems to struggle to find reasons for it by looking at the DFG and goes to Celonis to look into some example cases to find the reasons for not having a Payment activity. He performs root cause analysis. (27:00-31:30) Starts switching between Celonis and Disco and mentions having some difficulties with understanding the root cause analysis results (31:30-34:30) \\\n    Then he goes to the conformance overview and tries to mine the model with the Payment activity and compares the results of the conformance checking, going through the violation list but he is unsure about the result and asks to move on with the questions (34:30-39:00)\\\"\",\n  analysisExpertise: \"Average\",\n  tool: [\"Disco\", \"Celonis\"]\n},\n//Participant 3\n{\n  name: \"Participant 3\",\n  pmExpertise: \"Good\",\n  userStory: \"\\\"Starts by opening the event log as CSV to understand how the data is formatted and what it contains. (00:00-02:00)\\\n    Inspects the process map, looking at the activities and the main paths (02:00-06:30)\\\n    Opens the artifacts to have an idea of the context (06:30-11:00) \\\n    Then, she checks the guiding question and connects the \\\"\\\"reasons\\\"\\\" to the temporal constraints in the artifacts, which she thinks may hint to possible reasons (11:00-12:00)\\\n    She then explores the process map looking at the performance, finding bottlenecks (paths exhibiting long total and median duration) and rework. (13:00-18:00). She quickly checks the question to see if her findings answer it or not (18:00-18:30).\\\n    Then, she focuses on statistics [related to time] and starts looking at example cases in the variants, reflecting on the time constraints shown in the artifacts and the duration of transitions between activities [She never checks if the time constraints are respected or not] (18:30-27:00).\\\n    Notes down the first circumstance, i.e., that the police takes too long to send the fine (27:00-29:30)\\\n    Follows the same approach of looking at the variants and paths exhibiting long duration, finding the second circumstance in the delay between \\\"\\\"Add Penalty\\\"\\\" and \\\"\\\"Send for Credit Collection\\\"\\\" (19:30-32:00)\\\n    Then, she focuses on event attributes thinking that there must be something indicating how much the offender had to pay. She looks at the attribute distribution in Disco and notices that for one third of the cases the amount of the fine is empty. She thinks that the police is sending the fine without amount to pay and writes this as third circumstance (32:00-34:00).\\\n    \\\"\",\n  analysisExpertise: \"Average\",\n  tool: [\"Disco\"]\n},\n//Participant 4\n{\n  name: \"Participant 4\",\n  pmExpertise: \"Average\",\n  userStory: \"\\\"Starts by opening the event log as CSV to better understand the data and relate it to the question. He checks the guiding questions and decides to perform the analysis in Disco. (00:00-04:30)\\\n    In Disco, the participant starts by loading the event log for which he checks the artifacts again to decide which attributes he wants to include. He only loads a subset of the available attributes with the note to maybe adapt this later in the analysis, in case he notices that the selection is not enough to answer the question. (04:30-08:30)\\\n    After the event log is loaded, the participant reads and understands the activities in the process map. He is missing something and takes a few minutes to understand that there is no activity specific to missing payments and that he can find additional process variants not ending with the Payment activity by increasing the path-slider. (08:30-20:00)\\\n    After the analysis of the activities and variants, the participant explains to the interviewer how he would analyze the circumstances in more detail in PM4Py, in case he had more time (20:00-25:00).\\\n    To provide root causes of the circumstances, the participant uses ProM in which he tests a couple of methods: Inductive Visual Miner, Petri Net & Interactive Data-aware Heuristic Miner to identify \\\"\\\"this very explainable structure\\\"\\\" [understand the circumstances and provide root causes]. [He is not successful.] (25:00-29:00)\\\n    Afterwards, he decides to continues looking at the process map in Disco and takes notes regarding the three circumstances and his assumption regarding the root causes of the circumstances, solely based on the activities and variants displayed. (29:00-35:30)\\\n    The participant opens ProM again to test the Social Networking Mining in order to identify the most critical node for the variants leading to a missing payment. He create a Social Network (HoW) in ProM and notice that he missed to add a filter to the event log beforehand to be able to interpret results [not successful -  the interpretation is not clear and he is not able to identify any root causes]. (35:30 - 44:30)\\\n    The participant was stopped before performing a further analysis to continue with the questions & interview.\\\"\",\n  analysisExpertise: \"Average\",\n  tool: [\"Disco\", \"ProM\"]\n},\n//Participant 5\n{\n  name: \"Participant 5\",\n  pmExpertise: \"Average\",\n  userStory: \"\\\"After some connection problems, the participant starts from reading the artifacts and familiarize herself with the activities. Some activities and translation problems are clarified with help of the interviewer (00:00-08:00).\\\n    Afterwards, the analyst starts the analysis in the Process Explorer in Celonis, by following and understanding the activities. First, the visible connections are selected to a minima while in a second step she extends them, to get a feeling for the process (08:00-11:00).\\\n    Afterwards, the analyst searches for promising attributes for the analysis in the artifact documentation. She searches for the attributes to be able to compare amount to be paid vs. actual paid amount (11:00-14:30).\\\n    The analysts starts creating an OLAP table but seems to struggling with the selection of columns to be included in that table. She didn\\'t add any fields. [Replanning] She checks the documentation and the variant explorer in Celonis again. Additionally, she invests in understanding the different process variants and specially the end activities of the process (14:30-23:00). \\\n    [Replanning] She looks into the documents and reminds herself of the goals of the analysis. She explains her thought process about continuing the analysis with help of conformance checking. (23:00-27:30).\\\n    After another look at the Variant and Process Explorer, she switches to the Conformance View and starts adapting the to-be BPMN model. [A structured approach or a clear reason for the performed edits is not clear] [The analysis leads to no outcomes /insights about circumstances or root causes] (27:30 - 40:00).\\\n    She filters the process to cases with payment and uses the process overview to look at temporal aspects like the case distribution per month and the throughput time. Based on the case distribution, the analyst is able to identify a circumstance based on peaks in the distribution and ends the analysis (40:00-45:00)\\\"\",\n  analysisExpertise: \"Average\",\n  tool: [\"Celonis\"]\n},\n//Participant 6\n{\n  name: \"Participant 6\",\n  pmExpertise: \"Good\",\n  userStory: \"\\\"The participant starts by reading the artifacts aloud (00:00-04:00).  \\\n    After that she reads the guiding question and clarifies some open questions with the interviewer (04:00-06:30) .  \\\n    Then she opens Disco where she starts making sense of the control flow of the process and the involved activities by using the process map. She jumps back and forth between the activity descriptions in the artifacts and the process map. (06:00-13:00).  \\\n    While still using the process map, she starts to hypothesize about potential root causes [not data driven] about certain paths of the process (e.g. payment before fine is send). (13:00-14:00) \\\n    Now she focuses on the control-flow again and considers the frequency of events and the duration between events. She takes notes to capture her findings in between (14:00-18:30). \\\n    She dives deeper into performance related details and notices the temporal constraints within the process. [Even though the noticed the temporal constraints, it seems like she is not sure how to analyse it]. Still using the Process Map, she jumps between checking the performance, the number of events and temporal constraints/activity descriptions based on the provided artifacts. Before setting the filter, she mentioned that she is interested in checking if fines are not paid due to the late notification (18:30-24:00).\\\n    Therefore, she filters the data for a specific path . She uses the case view and observes that the payment still occurs in her selection, which she confirms in the Process Map (24:00-26:30). She switches back to the unfiltered event log where she checks further paths with and without payment and activities which can be followed after “Send Fine” (26:30-29:30). \\\n    [Brief interruption (29:30-33:00)].  \\\n    She decides to use performance metrics instead of frequency metrics to see the minimum and maximum durations [seems to not draw any conclusions based on the information]. She switches back to frequency metrics and changes the level of detail a couple of times [without any notably insights gained?] (33:00 - 37:30).  \\\n    She applies an endpoint filtering (“send fine”). By viewing the cases, she emits an hypothesis for a root cause of fines not being paid [fines are being sent too late because of a resource management problem,  which leads to a lot of fines not being paid] (37:30-45:00).  \\\n    She goes back to the unfiltered log and browses through the process map and explains using absolute frequency how many payments are not performed. She applies different levels of details and checks activities. [Seems like she is trying to come up with a valid hypothesis on activities following the payment, but everything seems a bit random] (45:00:56:00).\\\n    She uses the statistics view to check additional patterns and is interested in the number of events per case. (56:00-59:00).  \\\n    After going back to the process map and considering the flow at the full level of detail, she filters for cases being paid to understand the paths to payment. [Thanks to this step, she is able to understand why the payment activity is redundant in some cases]. (59:00-1:10).  \\\n    She later tries to explain why some cases are being sent to credit collection after payment [Unsuccessfully?] (1:10:00-1:16:30). \\\n     She starts to explain her main findings: (1) in a lot of cases, the fine is being sent very late, resulting in fines not being paid [the amount is unclear] (2) some payments are performed in two or three phases, (3) the system sometimes believes that the payment is not done, sending a message to credit collection [she assumes that it is an external process], while some partial payments do not require the credit collection step . [Connection interrupted] (1:16:30-1:19:30). \\\n    She finally switches to ProM, using the dotted chart to differentiate between activities, and finds that most are being sent to credit collection, which she confirms in Disco by observing the control flow of the process . She tries a similar approach for the payment activity but is stopped by time (1:19:30-1:27:00). \\\"\",\n  analysisExpertise: \"Good\",\n  tool: [\"Disco\"]\n},\n//Participant 7\n{\n  name: \"Participant 7\",\n  pmExpertise: \"Advanced\",\n  analysisExpertise: \"Average\",\n  tool: [\"Disco\"]\n},\n//Participant 8\n{\n  name: \"Participant 8\",\n  pmExpertise: \"Advanced\",\n  analysisExpertise: \"Good\",\n  tool: [\"Celonis\", \"ProM\"]\n},\n//Participant 9\n{\n  name: \"Participant 9\",\n  pmExpertise: \"Good\",\n  analysisExpertise: \"Good\",\n  tool: [\"Celonis\"]\n}];\nexport default participants;","map":{"version":3,"names":["participants","name","userStory","pmExpertise","analysisExpertise","tool"],"sources":["/Users/nicolaskesseli/NICOLAS_KESSELI/Programming/Lokal/promise-frontend/src/data/Participants.jsx"],"sourcesContent":["const participants = [\n  //Participant 1\n  {\n    name: \"Participant 1\",\n    userStory:\n    \"\\\"Starts by reading artifacts silently. (00:00-06:00)\\\n    Then he goes to Celonis and looks at the activities of the most common variants in the Variant Explorer and Process Explorer to familiarize, to get an initial understanding of the sequence of the process and the most common variants. He also makes a few assumptions about what he observes, for example \\\"\\\"so from Create Fine to Payment, this means that they received the fine directly on the place, right?\\\"\\\". (06:00-09:30)\\\n    Then, he looks at the research question to see what is expected and where to focus. (09:30-10:30)\\\n    He begins the analysis focusing on the Send for Credit Collection activity in Variant Explorer, which he had identified from the activity description as one circumstance for not paying. He filters for cases containing the Send for Credit Collection activity and explores variants. He then notices that some variants still contain the Payment activity and filters for cases without Payment. (10:30-14:00)\\\n    He mentions creating an OLAP table and looks at attributes to know what to put in, but quickly abandons that goal. (14:00-15:00) \\\n    [Replanning] Then, he decides to use the conformance checker as a fast solution to find reasons.\\\n    Since he doesn\\'t have a desired model, he uses the one discovered automatically by Celonis, removes the Send for Credit Collection activity to get the desired BPMN model and uses it as input for the conformance checker. Then he scrolls through the list of violations and stops at the first one, the Send for Credit Collection activity which was the one he wanted to drill down to. He performs root cause analysis for the Send for Credit Collection activity but doesn\\'t get any specific reason. Looks also at the other violations. (15:00-21:30)\\\n    [Replanning] He remembers about temporal constraints in the artifacts.\\\n    Applies throughput time selection filters to check the 90-day temporal constraint to find the answers. Then he moves on to check the temporal constraints about the prefecture. Checks the dismissal attribute and builds an OLAP table to check for dismissed cases. He\\'s satisfied with his findings. (21:30-33:00)\\\"\",\n    pmExpertise: \"Good\",\n    analysisExpertise: \"Basic\",\n    tool: [\"Celonis\"],\n  },\n\n  //Participant 2\n  {\n    name: \"Participant 2\",\n    pmExpertise: \"Average\",\n    userStory:\"\\\"Starts by reading the artifacts to get an understanding of the data (00:00-04:00) \\\n    Then he reads the guiding question (04:00-06:00). \\\n    Then he goes to Disco and looks the process map and general statistics to get a basic understanding of the process flow. (06:00 - 08:00). Then he switches between the guiding question and the artifacts attempting to find an activity that corresponds to not paying. Since he cannot find it, he looks into the attributes but then asks for help. The interviewer clarifies the question (08:00 - 13:00). \\\n    He continues searching in the attributes for something that could indicate not paying [seems not to know where to visualize attributes in Disco or raw data. He is encouraged by the interviewer]. (13:00-17:00) \\\n    Then he filters for cases that do not include the Payment activity and compares the DFG of the filtered event log in with the original one to find patterns [but he does not seem to find any] (17:00-24:30) \\\n    He asks the interviewer to clarify once more the concept of circumstance and decides that \\\"\\\"no Payment activity\\\"\\\" is then a circumstance. (24:00-27:00) \\\n    He seems to struggle to find reasons for it by looking at the DFG and goes to Celonis to look into some example cases to find the reasons for not having a Payment activity. He performs root cause analysis. (27:00-31:30) Starts switching between Celonis and Disco and mentions having some difficulties with understanding the root cause analysis results (31:30-34:30) \\\n    Then he goes to the conformance overview and tries to mine the model with the Payment activity and compares the results of the conformance checking, going through the violation list but he is unsure about the result and asks to move on with the questions (34:30-39:00)\\\"\",\n    analysisExpertise: \"Average\",\n    tool: [\"Disco\", \"Celonis\"],\n  },\n\n  //Participant 3\n  {\n    name: \"Participant 3\",\n    pmExpertise: \"Good\",\n    userStory:\"\\\"Starts by opening the event log as CSV to understand how the data is formatted and what it contains. (00:00-02:00)\\\n    Inspects the process map, looking at the activities and the main paths (02:00-06:30)\\\n    Opens the artifacts to have an idea of the context (06:30-11:00) \\\n    Then, she checks the guiding question and connects the \\\"\\\"reasons\\\"\\\" to the temporal constraints in the artifacts, which she thinks may hint to possible reasons (11:00-12:00)\\\n    She then explores the process map looking at the performance, finding bottlenecks (paths exhibiting long total and median duration) and rework. (13:00-18:00). She quickly checks the question to see if her findings answer it or not (18:00-18:30).\\\n    Then, she focuses on statistics [related to time] and starts looking at example cases in the variants, reflecting on the time constraints shown in the artifacts and the duration of transitions between activities [She never checks if the time constraints are respected or not] (18:30-27:00).\\\n    Notes down the first circumstance, i.e., that the police takes too long to send the fine (27:00-29:30)\\\n    Follows the same approach of looking at the variants and paths exhibiting long duration, finding the second circumstance in the delay between \\\"\\\"Add Penalty\\\"\\\" and \\\"\\\"Send for Credit Collection\\\"\\\" (19:30-32:00)\\\n    Then, she focuses on event attributes thinking that there must be something indicating how much the offender had to pay. She looks at the attribute distribution in Disco and notices that for one third of the cases the amount of the fine is empty. She thinks that the police is sending the fine without amount to pay and writes this as third circumstance (32:00-34:00).\\\n    \\\"\",\n    analysisExpertise: \"Average\",\n    tool: [\"Disco\"],\n  },\n\n  //Participant 4\n  {\n    name: \"Participant 4\",\n    pmExpertise: \"Average\",\n    userStory:\"\\\"Starts by opening the event log as CSV to better understand the data and relate it to the question. He checks the guiding questions and decides to perform the analysis in Disco. (00:00-04:30)\\\n    In Disco, the participant starts by loading the event log for which he checks the artifacts again to decide which attributes he wants to include. He only loads a subset of the available attributes with the note to maybe adapt this later in the analysis, in case he notices that the selection is not enough to answer the question. (04:30-08:30)\\\n    After the event log is loaded, the participant reads and understands the activities in the process map. He is missing something and takes a few minutes to understand that there is no activity specific to missing payments and that he can find additional process variants not ending with the Payment activity by increasing the path-slider. (08:30-20:00)\\\n    After the analysis of the activities and variants, the participant explains to the interviewer how he would analyze the circumstances in more detail in PM4Py, in case he had more time (20:00-25:00).\\\n    To provide root causes of the circumstances, the participant uses ProM in which he tests a couple of methods: Inductive Visual Miner, Petri Net & Interactive Data-aware Heuristic Miner to identify \\\"\\\"this very explainable structure\\\"\\\" [understand the circumstances and provide root causes]. [He is not successful.] (25:00-29:00)\\\n    Afterwards, he decides to continues looking at the process map in Disco and takes notes regarding the three circumstances and his assumption regarding the root causes of the circumstances, solely based on the activities and variants displayed. (29:00-35:30)\\\n    The participant opens ProM again to test the Social Networking Mining in order to identify the most critical node for the variants leading to a missing payment. He create a Social Network (HoW) in ProM and notice that he missed to add a filter to the event log beforehand to be able to interpret results [not successful -  the interpretation is not clear and he is not able to identify any root causes]. (35:30 - 44:30)\\\n    The participant was stopped before performing a further analysis to continue with the questions & interview.\\\"\",\n    analysisExpertise: \"Average\",\n    tool: [\"Disco\", \"ProM\"],\n  },\n\n  //Participant 5\n  {\n    name: \"Participant 5\",\n    pmExpertise: \"Average\",\n    userStory:\"\\\"After some connection problems, the participant starts from reading the artifacts and familiarize herself with the activities. Some activities and translation problems are clarified with help of the interviewer (00:00-08:00).\\\n    Afterwards, the analyst starts the analysis in the Process Explorer in Celonis, by following and understanding the activities. First, the visible connections are selected to a minima while in a second step she extends them, to get a feeling for the process (08:00-11:00).\\\n    Afterwards, the analyst searches for promising attributes for the analysis in the artifact documentation. She searches for the attributes to be able to compare amount to be paid vs. actual paid amount (11:00-14:30).\\\n    The analysts starts creating an OLAP table but seems to struggling with the selection of columns to be included in that table. She didn\\'t add any fields. [Replanning] She checks the documentation and the variant explorer in Celonis again. Additionally, she invests in understanding the different process variants and specially the end activities of the process (14:30-23:00). \\\n    [Replanning] She looks into the documents and reminds herself of the goals of the analysis. She explains her thought process about continuing the analysis with help of conformance checking. (23:00-27:30).\\\n    After another look at the Variant and Process Explorer, she switches to the Conformance View and starts adapting the to-be BPMN model. [A structured approach or a clear reason for the performed edits is not clear] [The analysis leads to no outcomes /insights about circumstances or root causes] (27:30 - 40:00).\\\n    She filters the process to cases with payment and uses the process overview to look at temporal aspects like the case distribution per month and the throughput time. Based on the case distribution, the analyst is able to identify a circumstance based on peaks in the distribution and ends the analysis (40:00-45:00)\\\"\",\n    analysisExpertise: \"Average\",\n    tool: [\"Celonis\"],\n  },\n\n  //Participant 6\n  {\n    name: \"Participant 6\",\n    pmExpertise: \"Good\",\n    userStory:\"\\\"The participant starts by reading the artifacts aloud (00:00-04:00).  \\\n    After that she reads the guiding question and clarifies some open questions with the interviewer (04:00-06:30) .  \\\n    Then she opens Disco where she starts making sense of the control flow of the process and the involved activities by using the process map. She jumps back and forth between the activity descriptions in the artifacts and the process map. (06:00-13:00).  \\\n    While still using the process map, she starts to hypothesize about potential root causes [not data driven] about certain paths of the process (e.g. payment before fine is send). (13:00-14:00) \\\n    Now she focuses on the control-flow again and considers the frequency of events and the duration between events. She takes notes to capture her findings in between (14:00-18:30). \\\n    She dives deeper into performance related details and notices the temporal constraints within the process. [Even though the noticed the temporal constraints, it seems like she is not sure how to analyse it]. Still using the Process Map, she jumps between checking the performance, the number of events and temporal constraints/activity descriptions based on the provided artifacts. Before setting the filter, she mentioned that she is interested in checking if fines are not paid due to the late notification (18:30-24:00).\\\n    Therefore, she filters the data for a specific path . She uses the case view and observes that the payment still occurs in her selection, which she confirms in the Process Map (24:00-26:30). She switches back to the unfiltered event log where she checks further paths with and without payment and activities which can be followed after “Send Fine” (26:30-29:30). \\\n    [Brief interruption (29:30-33:00)].  \\\n    She decides to use performance metrics instead of frequency metrics to see the minimum and maximum durations [seems to not draw any conclusions based on the information]. She switches back to frequency metrics and changes the level of detail a couple of times [without any notably insights gained?] (33:00 - 37:30).  \\\n    She applies an endpoint filtering (“send fine”). By viewing the cases, she emits an hypothesis for a root cause of fines not being paid [fines are being sent too late because of a resource management problem,  which leads to a lot of fines not being paid] (37:30-45:00).  \\\n    She goes back to the unfiltered log and browses through the process map and explains using absolute frequency how many payments are not performed. She applies different levels of details and checks activities. [Seems like she is trying to come up with a valid hypothesis on activities following the payment, but everything seems a bit random] (45:00:56:00).\\\n    She uses the statistics view to check additional patterns and is interested in the number of events per case. (56:00-59:00).  \\\n    After going back to the process map and considering the flow at the full level of detail, she filters for cases being paid to understand the paths to payment. [Thanks to this step, she is able to understand why the payment activity is redundant in some cases]. (59:00-1:10).  \\\n    She later tries to explain why some cases are being sent to credit collection after payment [Unsuccessfully?] (1:10:00-1:16:30). \\\n     She starts to explain her main findings: (1) in a lot of cases, the fine is being sent very late, resulting in fines not being paid [the amount is unclear] (2) some payments are performed in two or three phases, (3) the system sometimes believes that the payment is not done, sending a message to credit collection [she assumes that it is an external process], while some partial payments do not require the credit collection step . [Connection interrupted] (1:16:30-1:19:30). \\\n    She finally switches to ProM, using the dotted chart to differentiate between activities, and finds that most are being sent to credit collection, which she confirms in Disco by observing the control flow of the process . She tries a similar approach for the payment activity but is stopped by time (1:19:30-1:27:00). \\\"\",\n    analysisExpertise: \"Good\",\n    tool: [\"Disco\"],\n  },\n\n  //Participant 7\n  {\n    name: \"Participant 7\",\n    pmExpertise: \"Advanced\",\n    analysisExpertise: \"Average\",\n    tool: [\"Disco\"],\n  },\n\n  //Participant 8\n  {\n    name: \"Participant 8\",\n    pmExpertise: \"Advanced\",\n    analysisExpertise: \"Good\",\n    tool: [\"Celonis\", \"ProM\"],\n  },\n\n  //Participant 9\n  {\n    name: \"Participant 9\",\n    pmExpertise: \"Good\",\n    analysisExpertise: \"Good\",\n    tool: [\"Celonis\"],\n  },\n];\n\nexport default participants;\n"],"mappings":"AAAA,MAAMA,YAAY,GAAG;AACnB;AACA;EACEC,IAAI,EAAE,eAAe;EACrBC,SAAS,EACT;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6TAA6T;EACzTC,WAAW,EAAE,MAAM;EACnBC,iBAAiB,EAAE,OAAO;EAC1BC,IAAI,EAAE,CAAC,SAAS;AAClB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,SAAS;EACtBD,SAAS,EAAC;AACd;AACA;AACA;AACA;AACA;AACA;AACA,mRAAmR;EAC/QE,iBAAiB,EAAE,SAAS;EAC5BC,IAAI,EAAE,CAAC,OAAO,EAAE,SAAS;AAC3B,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,MAAM;EACnBD,SAAS,EAAC;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;EACHE,iBAAiB,EAAE,SAAS;EAC5BC,IAAI,EAAE,CAAC,OAAO;AAChB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,SAAS;EACtBD,SAAS,EAAC;AACd;AACA;AACA;AACA;AACA;AACA;AACA,mHAAmH;EAC/GE,iBAAiB,EAAE,SAAS;EAC5BC,IAAI,EAAE,CAAC,OAAO,EAAE,MAAM;AACxB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,SAAS;EACtBD,SAAS,EAAC;AACd;AACA;AACA;AACA;AACA;AACA,kUAAkU;EAC9TE,iBAAiB,EAAE,SAAS;EAC5BC,IAAI,EAAE,CAAC,SAAS;AAClB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,MAAM;EACnBD,SAAS,EAAC;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qUAAqU;EACjUE,iBAAiB,EAAE,MAAM;EACzBC,IAAI,EAAE,CAAC,OAAO;AAChB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,UAAU;EACvBC,iBAAiB,EAAE,SAAS;EAC5BC,IAAI,EAAE,CAAC,OAAO;AAChB,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,UAAU;EACvBC,iBAAiB,EAAE,MAAM;EACzBC,IAAI,EAAE,CAAC,SAAS,EAAE,MAAM;AAC1B,CAAC;AAED;AACA;EACEJ,IAAI,EAAE,eAAe;EACrBE,WAAW,EAAE,MAAM;EACnBC,iBAAiB,EAAE,MAAM;EACzBC,IAAI,EAAE,CAAC,SAAS;AAClB,CAAC,CACF;AAED,eAAeL,YAAY"},"metadata":{},"sourceType":"module","externalDependencies":[]}